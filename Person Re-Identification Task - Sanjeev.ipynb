{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b4c4306",
   "metadata": {},
   "source": [
    "## Person Re-Identification Task\n",
    "#### Name: Sanjeev Khannan\n",
    "\n",
    "For this person re-identification task, I grouped detection IDs into exactly 5 clusters, each representing a unique individual. To achieve this, I first normalized and reduce the dimensionality of the feature vectors from the detections. This preprocessing step helps to improve performance of the clustering by simplifying the data and ensures consistent feature scales. Next, I’ll use a Gaussian Mixture Model (GMM) for the initial clustering, as it’s effective at capturing complex patterns and overlapping clusters. And to evaluate the predicted clusters I wrote a custom evaluation technique to verify our predictions with true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9032ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e559b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detection data from the JSON file\n",
    "with open('final_detections.json', 'r') as f:\n",
    "    detections = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fddc6b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01484233, -0.05363799, -0.05816172, ..., -0.00557502,\n",
       "        -0.03075798, -0.00266532],\n",
       "       [-0.03776949, -0.02715539, -0.03124019, ..., -0.02639771,\n",
       "        -0.02372385, -0.01186128],\n",
       "       [-0.01462968, -0.02831179, -0.0522408 , ..., -0.00551042,\n",
       "        -0.04961643, -0.00492361],\n",
       "       ...,\n",
       "       [ 0.01070334,  0.00016948, -0.047523  , ..., -0.03672916,\n",
       "        -0.04369174, -0.00983843],\n",
       "       [ 0.01693735,  0.0021647 , -0.05257468, ..., -0.04102672,\n",
       "        -0.04835961, -0.01173849],\n",
       "       [ 0.01585637, -0.00551623, -0.05567126, ..., -0.03639167,\n",
       "        -0.04980672, -0.00640311]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract features and detection IDs\n",
    "features = []\n",
    "detection_ids = []\n",
    "\n",
    "for det in detections:\n",
    "    features.append(det['feature'])\n",
    "    detection_ids.append(det['detection_id'])\n",
    "\n",
    "features = np.array(features)\n",
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67283f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57868783, -0.57985488, -1.52276282, ...,  0.90700305,\n",
       "        -0.65727432, -0.00303677],\n",
       "       [-0.29531476,  0.84781311, -0.11710985, ..., -0.06168926,\n",
       "        -0.32965385, -0.36456722],\n",
       "       [ 0.58679419,  0.78547182, -1.21361385, ...,  0.91000819,\n",
       "        -1.53562211, -0.09181942],\n",
       "       ...,\n",
       "       [ 1.55250989,  2.32088789, -0.9672839 , ..., -0.54231892,\n",
       "        -1.25967481, -0.2850407 ],\n",
       "       [ 1.79015538,  2.4284497 , -1.23104705, ..., -0.74224566,\n",
       "        -1.4770847 , -0.35973994],\n",
       "       [ 1.7489475 ,  2.01437325, -1.39272829, ..., -0.52661842,\n",
       "        -1.54448492, -0.14998462]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Normalize the features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a9b5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11000, 256)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensionality Reduction with PCA\n",
    "pca = PCA(n_components=256)  # Reduce to 256 dimensions for better clustering performance\n",
    "features = pca.fit_transform(features)\n",
    "features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1509fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 1, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform clustering with Gaussian Mixture Models\n",
    "# Since we have data of 5 persons, setting num_clusters to 5\n",
    "num_clusters = 5\n",
    "gmm = GaussianMixture(n_components=num_clusters, random_state=0)\n",
    "labels = gmm.fit_predict(features)\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "943203f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing to ensure exactly 5 clusters\n",
    "# Map cluster labels to detection IDs\n",
    "clusters = {i: [] for i in range(num_clusters)}\n",
    "for i, label in enumerate(labels):\n",
    "    clusters[label].append(detection_ids[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3433f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output in the format required\n",
    "prediction = list(clusters.values())\n",
    "\n",
    "# Step 7: Save the prediction results to a JSON file\n",
    "with open('prediction.json', 'w') as f:\n",
    "    json.dump(prediction, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c38289",
   "metadata": {},
   "source": [
    "## Inspection of labels.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa05ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample data (assuming you have loaded it already)\n",
    "with open('labels.json') as f:\n",
    "    true_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5939c79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[113, 90, 113, 112, 132]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(sub_l) for sub_l in true_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6766c519",
   "metadata": {},
   "source": [
    "### In the detections data, there are 11000 detections and the task is to group all the similar person's detections, but in the labels.json file there were only 560 detections totally. I assume you gave a partial data for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cde1b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[567, 447, 567, 2260, 2503, 2508, 2743, 2748, 2979, 2984, 3184, 3188, 3376, 3607, 4924, 5135, 5141, 5301, 5306, 5461, 5465, 5636, 5642, 7749, 8192, 8335, 332, 677, 792, 1210, 1816, 2036, 2038, 2263, 2268, 2500, 3372, 3612, 3867, 3871, 4116, 4711, 4930, 5839, 6004, 6541, 7626, 7850, 7943, 8055, 8056, 795, 912, 915, 1050, 1818, 1598, 1396, 1390, 2043, 4108, 6160, 6332, 8495, 8497, 8653, 1054, 1605, 8194, 1214, 1823, 8337, 7484, 8781, 8923, 8650, 9066, 8778, 5138, 6865, 6534, 2975, 6534, 2740, 2740, 6325, 5833, 5833, 8919, 8919]\n"
     ]
    }
   ],
   "source": [
    "print(true_labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5566c1",
   "metadata": {},
   "source": [
    "### And also there are some duplicate entries in the labels.json, in the above group as you can see, '567' detection_id is present twice in the list. If detection_id is unique as mentioned in the problem statement, how there can be duplicate entries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838583b9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa2d88",
   "metadata": {},
   "source": [
    "## Evaluation Technique\n",
    "Since there are lot of confusions in the labels.json, I will use a custom evaluation technique to compare the cluster_id of a detection in the lables.json with our predictions.\n",
    "\n",
    "For each matched clusters, it will marked as 1 and 0 for mismatch. This will give us how perfect is our cluster predictions compared to limited labels.json data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d4a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping each detection id with a cluster_number\n",
    "prediction_map = {}\n",
    "\n",
    "for cluster_id, detection_id_list in enumerate(prediction):\n",
    "    for detection_id in detection_id_list:\n",
    "        prediction_map[detection_id]=cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59aa23c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each detection_id is assigned a cluster_number\n",
    "len(prediction_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1152d8f",
   "metadata": {},
   "source": [
    "Now for each true_labels clusters,\n",
    "- we will take first `detection_id` and note the `cluster_id` from our prediction_map.\n",
    "- Now check all the remaining detection_ids match the same `cluster_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a60875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_prediction_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for true_detection_list in true_labels:\n",
    "    expected_cluster_id = prediction_map[true_detection_list[0]]\n",
    "\n",
    "    for detection_id in true_detection_list[1:]:\n",
    "        if expected_cluster_id == prediction_map[detection_id]:\n",
    "            matched_prediction_count+=1\n",
    "        total_count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eee3a68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467, 555)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_prediction_count, total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed1893",
   "metadata": {},
   "source": [
    "I got around 467 correct predictions out of 555."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ffc9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy of the clustering - 0.8414414414414414\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average accuracy of the clustering - {matched_prediction_count/total_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb3c09",
   "metadata": {},
   "source": [
    "## I got around 84% accuracy for person re-identification grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7045d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
